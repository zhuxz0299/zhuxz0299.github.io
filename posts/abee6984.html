<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>Multilayer perceptrons | Zhuxz's Blog</title><meta name="author" content="朱小志"><meta name="copyright" content="朱小志"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="多层感知机，过拟合和欠拟合，权重衰退，丢弃法，数值稳定性">
<meta property="og:type" content="article">
<meta property="og:title" content="Multilayer perceptrons">
<meta property="og:url" content="http://zhuxz0299.github.io/posts/abee6984.html">
<meta property="og:site_name" content="Zhuxz&#39;s Blog">
<meta property="og:description" content="多层感知机，过拟合和欠拟合，权重衰退，丢弃法，数值稳定性">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://source.fomal.cc/img/default_cover_8.webp">
<meta property="article:published_time" content="2023-09-01T07:12:01.000Z">
<meta property="article:modified_time" content="2023-09-03T04:20:23.564Z">
<meta property="article:author" content="朱小志">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://source.fomal.cc/img/default_cover_8.webp"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://zhuxz0299.github.io/posts/abee6984.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: false,
    rightside: true,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Multilayer perceptrons',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-09-03 12:20:23'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="/css/double_row_bug_fix.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="/css/category_card.css"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">62</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://source.fomal.cc/img/default_cover_8.webp')"><nav id="nav"><span id="blog-info"><a href="/" title="Zhuxz's Blog"><span class="site-name">Zhuxz's Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Multilayer perceptrons</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-09-01T07:12:01.000Z" title="发表于 2023-09-01 15:12:01">2023-09-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-03T04:20:23.564Z" title="更新于 2023-09-03 12:20:23">2023-09-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">动手学深度学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Multilayer perceptrons"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h1><h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><p>人工智能最早的模型。给定输入 $\bm{x}$，权重 $\bm{w}$，和偏移 $b$，感知机输出</p>
<script type="math/tex; mode=display">
o = \sigma(<\bm{w},\bm{x}>+b) \quad \sigma(x) = \begin{cases}
    1, \text{if } x>0 \\ -1, \text{otherwise}
\end{cases}</script><p>感知机在这里就相当于是一个二分类的问题。</p>
<h3 id="训练感知机"><a href="#训练感知机" class="headerlink" title="训练感知机"></a>训练感知机</h3><p>如果 $y_i[<w, x_i>+b]\le 0$，那么 $w \leftarrow w+y_i x_i, b \leftarrow b+y_i$，知道所有类都分类正确。</p>
<p>这种算法等价于使用批量大小为 $1$ 的梯度下降，并且使用的损失函数为 $l(y, \bm{x}, \bm{w})=\max (0, -y&lt;\bm{w}, \bm{x}&gt;)$。</p>
<h3 id="收敛定理"><a href="#收敛定理" class="headerlink" title="收敛定理"></a>收敛定理</h3><p>因为感知机是一个很简单的模型，因此它有一个很好的收敛定理。</p>
<p>假设数据在半径 $r$ 内，$\exists \rho&gt;0$，使得对于 $\left| \bm{w} \right|_{}^{2}b^{2}\le 1$，有 $y(\bm{x}^{\mathrm{T}}\bm{w}+b)\ge \rho$。那么感知机保证能在 $\displaystyle \frac{r^{2}+1}{\rho^{2}}$ 步后收敛。</p>
<h3 id="XOR问题"><a href="#XOR问题" class="headerlink" title="XOR问题"></a>XOR问题</h3><p>感知机不能拟合XOR问题，因为它只能产生线性的分割面。<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src='../../figure/动手学深度学习/XOR_problem.png' width=250 style="display: block; margin-left: auto; margin-right: auto;"></p>
<p>比如上图所示的分类问题就无法用感知机直接解决。</p>
<h2 id="多层感知机-1"><a href="#多层感知机-1" class="headerlink" title="多层感知机"></a>多层感知机</h2><p>从对感知机的讨论中，我们可以看出简单的线性模型无法解决XOR问题。因此我们考虑先学习蓝色的线，再学习黄色的线，将两个进行组合，从而得到正确的结果。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src='../../figure/动手学深度学习/XOR_problem_solver.png' width=200 style="display: block; margin-left: auto; margin-right: auto;"></p>
<h3 id="单隐藏层——单分类"><a href="#单隐藏层——单分类" class="headerlink" title="单隐藏层——单分类"></a>单隐藏层——单分类</h3><ul>
<li>输入层 $\bm{x}\in \mathbb{R}^{n}$</li>
<li>隐藏层 $\bm{W_1} \in \mathbb{R}^{m\times n}, \bm{b_1}\in \mathbb{R}^{m}$</li>
<li>输出层 $\bm{w_2}\in \mathbb{R}^{m}, b_2\in \mathbb{R}$</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
\bm{h} &= \sigma (\bm{W_1}\bm{x}+\bm{b_1})\\
o&=\bm{w_2}^{\mathrm{T}}\bm{h}+b_2
\end{aligned}</script><p>$\sigma$ 是一个按元素的非线性激活函数。激活函数不能是线性的，否则会导致输出 $o$ 依然是一个相对 $\bm{x}$ 的线性函数，那么没有隐藏层的感知机就没有区别了。</p>
<h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><h4 id="Sigmoid-激活函数"><a href="#Sigmoid-激活函数" class="headerlink" title="Sigmoid 激活函数"></a>Sigmoid 激活函数</h4><script type="math/tex; mode=display">
\operatorname{sigmoid}(x) = \frac{1}{1+\exp (-x)}</script><h4 id="Tanh函数"><a href="#Tanh函数" class="headerlink" title="Tanh函数"></a>Tanh函数</h4><script type="math/tex; mode=display">
\tanh(x)=\frac{1-\exp (-2x)}{1+\exp (2x)}</script><h4 id="ReLU激活函数"><a href="#ReLU激活函数" class="headerlink" title="ReLU激活函数"></a>ReLU激活函数</h4><script type="math/tex; mode=display">
\operatorname{ReLU}(x) = \max (x, 0)</script><p>ReLU的优势在于比较简单，计算足够快。</p>
<h3 id="多类分类"><a href="#多类分类" class="headerlink" title="多类分类"></a>多类分类</h3><p>和 Softmax 基本一致，只是多了个隐藏层。</p>
<ul>
<li>输入层 $\bm{x}\in \mathbb{R}^{n}$</li>
<li>隐藏层 $\bm{W_1} \in \mathbb{R}^{m\times n}, \bm{b_1}\in \mathbb{R}^{m}$</li>
<li>输出层 $\bm{W_2}\in \mathbb{R}^{m\times k}, b_2\in \mathbb{R}^{k}$</li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned}
\bm{h} &= \sigma (\bm{W_1}\bm{x}+\bm{b_1})\\
\bm{o}&=\bm{W_2}^{\mathrm{T}}\bm{h}+\bm{b_2} \\
\bm{y}&=\operatorname{softmax}(\bm{o})
\end{aligned}</script><h3 id="多隐藏层"><a href="#多隐藏层" class="headerlink" title="多隐藏层"></a>多隐藏层</h3><p>在中间多加几层就行。</p>
<p>多隐藏层的每一层的大小一般慢慢减小，不断地提取信息。同时最下面的隐藏层也可以相对输入稍微大一些。<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src='../../figure/动手学深度学习/multi_hidden_layers.png' width=400 style="display: block; margin-left: auto; margin-right: auto;"></p>
<h2 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h2><ul>
<li>为什么神经网络倾向于增加网络的层数而不是某一层神经元的个数？<ul>
<li>在相同计算量的情况下，更深的网络更好训练一些。浅层但是很胖的网络理论效果应该和计算量相同的深度网络一样，但是这需要一次学习大量数据特征，不好训练，容易导致过拟合；而深层的网络可以理解为一步步地学习数据特征。</li>
</ul>
</li>
<li>relu为什么管用？<ul>
<li>激活函数的本质就是引入非线性性，因此对激活函数的复杂性没有太多要求。</li>
</ul>
</li>
<li>如何设置模型的深度和宽度？<ul>
<li>遇到一个问题，可以先直接使用线性模型算一次，然后在加一层，并且试出一个比较好的结果，再不断向上增加隐藏层，继续尝试。</li>
</ul>
</li>
</ul>
<h1 id="模型选择-过拟合和欠拟合"><a href="#模型选择-过拟合和欠拟合" class="headerlink" title="模型选择+过拟合和欠拟合"></a>模型选择+过拟合和欠拟合</h1><h2 id="模型选择"><a href="#模型选择" class="headerlink" title="模型选择"></a>模型选择</h2><h3 id="训练误差和泛化误差"><a href="#训练误差和泛化误差" class="headerlink" title="训练误差和泛化误差"></a>训练误差和泛化误差</h3><ul>
<li>训练误差：模型在训练数据上的误差</li>
<li>泛化误差：模型在新数据上的误差</li>
</ul>
<h3 id="验证数据集和测试数据集"><a href="#验证数据集和测试数据集" class="headerlink" title="验证数据集和测试数据集"></a>验证数据集和测试数据集</h3><ul>
<li>验证数据集：一个用来评估模型好坏的数据集<ul>
<li>帮助调整超参数</li>
</ul>
</li>
<li>测试数据集：只用一次的数据集</li>
</ul>
<h3 id="K-则交叉验证"><a href="#K-则交叉验证" class="headerlink" title="K-则交叉验证"></a>K-则交叉验证</h3><p>在没有足够多的数据时使用</p>
<ul>
<li>将数据分成 $K$ 块<ul>
<li>使用第 $i$ 块作为验证数据集，其余的作为训练数据集</li>
</ul>
</li>
<li>报告 $K$ 个验证集误差的平均</li>
</ul>
<p>常用：$K=5$ 或 $10$</p>
<h2 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src='../../figure/动手学深度学习/overfitting_and_underfitting.png' width=400 style="display: block; margin-left: auto; margin-right: auto;"></p>
<h3 id="模型容量"><a href="#模型容量" class="headerlink" title="模型容量"></a>模型容量</h3><ul>
<li>低容量的模型难以拟合训练数据</li>
<li>高容量的模型可以记住所有的训练数据</li>
</ul>
<p>在学习模型的时候我们希望得到一个最小的泛化误差<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src='../../figure/动手学深度学习/influence_of_model_capacity.png' width=400 style="display: block; margin-left: auto; margin-right: auto;"></p>
<h4 id="估计模型容量"><a href="#估计模型容量" class="headerlink" title="估计模型容量"></a>估计模型容量</h4><ul>
<li>难以在不同的种类算法之间比较<ul>
<li>例如树模型和神经网络</li>
</ul>
</li>
<li>给定一个模型种类，将有两个因素<ul>
<li>参数的个数</li>
<li>参数值的选择范围</li>
</ul>
</li>
</ul>
<h4 id="VC维"><a href="#VC维" class="headerlink" title="VC维"></a>VC维</h4><p>VC维是统计学习理论的核心思想</p>
<p>对于一个分类模型，VC等于一个最大的数据集的大小，不管如何给定标号，都存在一个模型来对它进行完美分类（相当于记住所有数据）。</p>
<ul>
<li>二维输入的感知机，VC维 $=3$（即假如数据集只有 $3$ 个样本，那么感知机总是能进行划分；但是如果有 $4$ 个样本，那对于比如说XOR的情形就无法处理）</li>
<li>支持 $N$ 维输入的感知机的VC维是 $N+1$</li>
<li>一些多层感知机的VC维为 $O(N\log_2 N)$</li>
</ul>
<h3 id="数据复杂度"><a href="#数据复杂度" class="headerlink" title="数据复杂度"></a>数据复杂度</h3><ul>
<li>样本个数</li>
<li>每个样本的元素个数</li>
<li>时间、空间结构</li>
<li>多样性</li>
</ul>
<h2 id="QA-1"><a href="#QA-1" class="headerlink" title="QA"></a>QA</h2><ul>
<li>验证数据集和训练数据集的数据清洗（如异常值处理）和特征构建（如标准化）是否需要放到一起处理？<ul>
<li>如果能拿到验证数据集，那么可以统一做处理；如果拿不到，那就单独处理训练集的数据。</li>
</ul>
</li>
<li>如何有效地设计超参数？<ul>
<li>自己多次调试，或者随机。</li>
</ul>
</li>
<li>假设做一个二分类问题，实际情况是1/9的比例，训练集两种类型的比例应该是1/1还是1/9?<ul>
<li>验证集的数据比例最好是比较均衡，或者也可以调整权重</li>
</ul>
</li>
</ul>
<h1 id="权重衰退"><a href="#权重衰退" class="headerlink" title="权重衰退"></a>权重衰退</h1><h2 id="权重衰退-1"><a href="#权重衰退-1" class="headerlink" title="权重衰退"></a>权重衰退</h2><p>一种处理过拟合的方法。权重衰退是通过限制参数值的选择范围来控制模型容量。<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src='../../figure/动手学深度学习/weight_decay_parameter_restriction.png' width=200 style="display: block; margin-left: auto; margin-right: auto;"></p>
<p>限制参数的范围之所以能防止过拟合，是因为较小的范围能够防止神经网络学习出一些过于复杂的模型，从而使学习的结果趋于平滑。就如上图所示。</p>
<h3 id="使用均方范数作为硬性限制"><a href="#使用均方范数作为硬性限制" class="headerlink" title="使用均方范数作为硬性限制"></a>使用均方范数作为硬性限制</h3><script type="math/tex; mode=display">
\min_{}l(\bm{w},b) \text{ subject to } \left\| \bm{w} \right\|_{}^{2}\le \theta</script><ul>
<li>通常不限制 $b$</li>
<li>小的 $\theta$ 意味着更强的正则项</li>
</ul>
<h3 id="使用均方范数作为柔性限制"><a href="#使用均方范数作为柔性限制" class="headerlink" title="使用均方范数作为柔性限制"></a>使用均方范数作为柔性限制</h3><p>对于每个 $\theta$，都可以找到 $\lambda$ 使得之前的目标函数等价于（可以通过拉格朗日乘子法证明）</p>
<script type="math/tex; mode=display">
\min l(\bm{w},b)+\frac{\lambda}{2}\left\| \bm{w} \right\|_{}^{2}</script><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src='../../figure/动手学深度学习/weight_decay_illustration.png' width=400 style="display: block; margin-left: auto; margin-right: auto;"></p>
<h3 id="参数更新法则"><a href="#参数更新法则" class="headerlink" title="参数更新法则"></a>参数更新法则</h3><ul>
<li>计算梯度<script type="math/tex; mode=display">
\frac{\partial }{\partial \bm{w}} \left(l(\bm{w},b)+\frac{\lambda}{2}\left\| \bm{w} \right\|_{}^{2}\right) = \frac{\partial l(\bm{w},b)}{\partial \bm{w}} + \lambda \bm{w}</script></li>
<li>时间 $t$ 的更新<script type="math/tex; mode=display">
\bm{w_{t+1}}=(1-\eta \lambda)\bm{w_t}-\eta \frac{\partial l(\bm{w_t},b)}{\partial \bm{w_t}}</script></li>
</ul>
<p>从式子中可以看出在更新 $\bm{w}$ 的时候总是先将 $\bm{w}$ 变小再减去梯度，所以叫做权重衰退。</p>
<h2 id="QA-2"><a href="#QA-2" class="headerlink" title="QA"></a>QA</h2><ul>
<li>实践中权重衰退的值一般取多少比较合适？<ul>
<li>一般取 $10^{-2}, 10^{-3}, 10^{-4}$。效果不好可以取其他方法。</li>
</ul>
</li>
</ul>
<h1 id="丢弃法"><a href="#丢弃法" class="headerlink" title="丢弃法"></a>丢弃法</h1><h2 id="丢弃法-1"><a href="#丢弃法-1" class="headerlink" title="丢弃法"></a>丢弃法</h2><p>一个好的模型需要对输入数据的扰动鲁棒</p>
<ul>
<li>使用有噪音的数据等价于Tikhonov正则（这里的噪音和数据集的噪音不同，数据集的噪音是固定的，因此可能被学习到；而这里正则加入的噪音是随机的）</li>
<li>丢弃法就相当于在层之间加噪音</li>
</ul>
<h3 id="无偏差的加入噪音"><a href="#无偏差的加入噪音" class="headerlink" title="无偏差的加入噪音"></a>无偏差的加入噪音</h3><p>对某一层 $\bm{x}$ 加入噪音得到 $\bm{x’}$，但我们希望期望值不变 $E(\bm{x’})=E(\bm{x})$，于是丢弃法对每个元素进行如下扰动：</p>
<script type="math/tex; mode=display">
x_i'=\begin{cases}
    0, \text{with probablity } p \\
    \frac{x_i}{1-p}, \text{oterwise}
\end{cases}</script><h3 id="使用丢弃法"><a href="#使用丢弃法" class="headerlink" title="使用丢弃法"></a>使用丢弃法</h3><p>通常将丢弃法作用在隐藏全连接层的输出上</p>
<script type="math/tex; mode=display">
\begin{aligned}
\bm{h}&=\sigma(\bm{W_1}\bm{x}+\bm{b_1}) \\
\bm{h'}&=\operatorname{dropout}(\bm{h}) \\
\bm{o}&=\bm{W_2}\bm{h'}+\bm{b_2}\\
\bm{y}&=\operatorname{softmax}(\bm{o})
\end{aligned}</script><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src='../../figure/动手学深度学习/using_dropout.png' width=500 style="display: block; margin-left: auto; margin-right: auto;"></p>
<p>需要注意的是每次dropout丢弃的神经元都是随机的。</p>
<p>同时dropout只在训练时使用，用来影响模型的更新；而在推理时不需要drop。</p>
<h2 id="QA-3"><a href="#QA-3" class="headerlink" title="QA"></a>QA</h2><ul>
<li>为什么预测的时候不用drop？<ul>
<li>为了防止推理时出现随机性</li>
</ul>
</li>
<li>dropout为什么比权重衰退更好用更常用？<ul>
<li>dropout更加直观，方便调参</li>
</ul>
</li>
</ul>
<h1 id="数值稳定性-模型初始化和激活函数"><a href="#数值稳定性-模型初始化和激活函数" class="headerlink" title="数值稳定性+模型初始化和激活函数"></a>数值稳定性+模型初始化和激活函数</h1><h2 id="数值稳定性"><a href="#数值稳定性" class="headerlink" title="数值稳定性"></a>数值稳定性</h2><h3 id="神经网络的梯度"><a href="#神经网络的梯度" class="headerlink" title="神经网络的梯度"></a>神经网络的梯度</h3><p>损失函数 $l$ 关于参数 $\bm{W_t}$ 的梯度</p>
<script type="math/tex; mode=display">
\frac{\partial l}{\partial \bm{W_t}} = \frac{\partial l}{\partial \bm{h^{d}}} \frac{\partial \bm{h^{d}}}{\partial \bm{h^{d-1}}}\cdots \frac{\partial \bm{h^{t+1}}}{\partial \bm{h^{t}}} \frac{\partial \bm{h^{t}}}{\partial \bm{W^{t}}}</script><p>假如神经网络非常深，那么可能带来梯度爆炸或者梯度消失。</p>
<h3 id="梯度爆炸的问题"><a href="#梯度爆炸的问题" class="headerlink" title="梯度爆炸的问题"></a>梯度爆炸的问题</h3><ul>
<li>值超出值域</li>
<li>对学习率敏感<ul>
<li>如果学习率太大 $\rightarrow$ 大的参数值 $\rightarrow$ 更大的梯度</li>
<li>学习率太小 $\rightarrow$ 训练无法进展</li>
<li>我们可能需要在训练过程中不断调整学习率</li>
</ul>
</li>
</ul>
<h3 id="梯度消失的问题"><a href="#梯度消失的问题" class="headerlink" title="梯度消失的问题"></a>梯度消失的问题</h3><ul>
<li>梯度值变成 $0$<ul>
<li>训练没有进展</li>
</ul>
</li>
<li>对底部层尤为严重<ul>
<li>仅仅顶部训练的较好</li>
<li>无法让神经网络更深</li>
</ul>
</li>
</ul>
<h2 id="模型初始化和激活函数"><a href="#模型初始化和激活函数" class="headerlink" title="模型初始化和激活函数"></a>模型初始化和激活函数</h2><p>目标：让训练更加稳定，即让梯度值在合理的范围内</p>
<ul>
<li>将乘法变为加法<ul>
<li>ResNet，LSTM</li>
</ul>
</li>
<li>归一化<ul>
<li>梯度归一化，梯度剪裁（即假如梯度超过了某个范围，就强行把梯度剪裁回去）</li>
</ul>
</li>
</ul>
<h3 id="让每层的方差是一个常数"><a href="#让每层的方差是一个常数" class="headerlink" title="让每层的方差是一个常数"></a>让每层的方差是一个常数</h3><p>我们把每层的输出和梯度都看成是一个随机变量，那么我们希望每层的方差和均值都保持一致，那么神经网络不论多深，都不容易出问题。</p>
<p>令正向</p>
<script type="math/tex; mode=display">
\mathbb{E}[h_i^{t}] = 0 \quad \operatorname{Var}[h_i^{t}]=a</script><p>反向</p>
<script type="math/tex; mode=display">
\mathbb{E}\left[\frac{\partial l}{\partial h_i^{t}}\right] = 0 \quad \operatorname{Var}\left[ \frac{\partial l}{\partial h_i^{t}} \right]=b</script><h3 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h3><p>即在合理值区间里随机初始参数。一般而言，远离最优解的地方损失函数表面会比较复杂，而最优解附近表面会比较平。之前随机初始化 $N(0, 0.01)$ 对于小网络是可行的，但是不一定适用于深度神经网络。</p>
<p>以MLP为例。假设</p>
<ul>
<li>$w<em>{i,j}^{t}$ i.i.d，那么 $\mathbb{E}[w</em>{i,j}^{t}]=0, \operatorname{Var}[w_{i,j}^{t}]=\gamma_t$</li>
<li>$h<em>i^{t-1}$ 独立于 $w</em>{i,j}^{t}$</li>
</ul>
<p>同时先不考虑激活函数，那么 $\bm{h}^{t}=\bm{W}^{t}\bm{h}^{t-1}$</p>
<script type="math/tex; mode=display">
\mathbb{E}[h_i^{t}]=\mathbb{E}\left[ \sum_{j}w_{i,j}^{t}h_j^{t-1} \right] = \sum_{j}\mathbb{E}[w_{i,j}^{t}]\mathbb{E}[h_j^{t-1}]=0</script><script type="math/tex; mode=display">
\begin{aligned}
\operatorname{Var}\left[h_{i}^{t}\right] & =\mathbb{E}\left[\left(h_{i}^{t}\right)^{2}\right]-\mathbb{E}\left[h_{i}^{t}\right]^{2}=\mathbb{E}\left[\left(\sum_{j} w_{i, j}^{t} h_{j}^{t-1}\right)^{2}\right] \\
& =\mathbb{E}\left[\sum_{j}\left(w_{i, j}^{t}\right)^{2}\left(h_{j}^{t-1}\right)^{2}+\sum_{j \neq k} w_{i, j}^{t} w_{i, k}^{t} h_{j}^{t-1} h_{k}^{t-1}\right] \\
& =\sum_{j} \mathbb{E}\left[\left(w_{i, j}^{t}\right)^{2}\right] \mathbb{E}\left[\left(h_{j}^{t-1}\right)^{2}\right] \\
& =\sum_{j} \operatorname{Var}\left[w_{i, j}^{t}\right] \operatorname{Var}\left[h_{j}^{t-1}\right]=n_{t-1} \gamma_{t} \operatorname{Var}\left[h_{j}^{t-1}\right]
\end{aligned} \Rightarrow n_{t-1}\gamma_t=1</script><p>再考虑反向</p>
<script type="math/tex; mode=display">
\frac{\partial \ell}{\partial \mathbf{h}^{t-1}}=\frac{\partial \ell}{\partial \mathbf{h}^{t}} \mathbf{W}^{t} \Rightarrow \left(\frac{\partial \ell}{\partial \mathbf{h}^{t-1}}\right)^{T}=\left(W^{t}\right)^{T}\left(\frac{\partial \ell}{\partial \mathbf{h}^{t}}\right)^{T} \\ \quad \\
\mathbb{E}\left[\frac{\partial \ell}{\partial h_{i}^{t-1}}\right]=0 \\ \quad \\
\operatorname{Var}\left[\frac{\partial \ell}{\partial h_{i}^{t-1}}\right]=n_{t} \gamma_{t} \operatorname{Var}\left[\frac{\partial \ell}{\partial h_{j}^{t}}\right] \Rightarrow n_{t} \gamma_{t}=1</script><h3 id="Xavier初始"><a href="#Xavier初始" class="headerlink" title="Xavier初始"></a>Xavier初始</h3><p>难以同时满足 $n_{t-1}\gamma_t=1$ 和 $n_t\gamma_t=1$，于是取个折中的方案</p>
<ul>
<li>Xavier使得 $\gamma<em>t(n</em>{t-1}+n<em>t)/2=1\rightarrow \gamma_t=2/(n</em>{t-1}+n_t)$<ul>
<li>均匀分布 $\mathscr{U}(- \sqrt{6 /(n<em>{t-1}+n_t)}, \sqrt{6 /(n</em>{t-1}+n_t)})$</li>
<li>正态分布 $\mathscr{N}(0, \sqrt{2 /(n_{t-1}+n_t)})$</li>
</ul>
</li>
</ul>
<h3 id="假设线性的激活函数"><a href="#假设线性的激活函数" class="headerlink" title="假设线性的激活函数"></a>假设线性的激活函数</h3><p>假设 $\sigma(x)=\alpha x+\beta$</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\mathbf{h}'=\mathbf{W}^t\mathbf{h}^{t-1}\quad\text{and}\quad\mathbf{h}^t=\sigma(\mathbf{h}') \\
\mathbb{E}[h_i^t]& =\mathbb{E}\left[\alpha h_i^{\prime}+\beta\right]=\beta \Rightarrow \beta=0 \\
\mathbf{Var}[h_i^t]& =\mathbb{E}[(h_i^t)^2]-\mathbb{E}[h_i^t]^2  \\
&=\mathbb{E}[(\alpha h_i^{\prime}+\beta)^2]-\beta^2 \\
&=\mathbb{E}[\alpha^2(h_i^{\prime})^2+2\alpha\beta h_i^{\prime}+\beta^2]-\beta^2 \\
&=\alpha^2\mathrm{Var}[h_i^{\prime}] \Rightarrow \alpha=1
\end{aligned}</script><p>在反向的情况下</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial\ell}{\partial\mathbf{h}^{\prime}}&=\frac{\partial\ell}{\partial\mathbf{h}^{\prime}}(W^t)^T\quad\mathrm{and}\quad\frac{\partial\ell}{\partial\mathbf{h}^{t-1}}=\alpha\frac{\partial\ell}{\partial\mathbf{h}^{\prime}}\\\\
&\mathbb{E}\left[\frac{\partial\ell}{\partial h_i^{t-1}}\right]=0\quad \Rightarrow \quad\beta=0\\\\
\mathrm{Var}\left[\frac{\partial\ell}{\partial h_i^{t-1}}\right]&=\alpha^2\mathrm{Var}\left[\frac{\partial\ell}{\partial h_j^{\prime}}\right]\quad \Rightarrow \quad\alpha=1
\end{aligned}</script><h3 id="检查常用激活函数"><a href="#检查常用激活函数" class="headerlink" title="检查常用激活函数"></a>检查常用激活函数</h3><p>通过上面的讨论我们发现激活函数最好是 $f(x)=x$，单这对非线性的激活函数来说是不可能的，因此我们讨论泰勒展开的情况。</p>
<script type="math/tex; mode=display">
\begin{gathered}
\text{sigmoid(x)} =\frac12+\frac x4-\frac{x^3}{48}+O(x^5) \\
\text{tanh(x)} =0+x-\frac{x^3}3+O(x^5) \\
\text{relu(x)} =0+x\quad\mathrm{~for~}x\geq0 
\end{gathered}</script><p>发现sigmoid不太符合，因此可以调整为</p>
<script type="math/tex; mode=display">
4\times \operatorname{sigmoid} - 2</script><h2 id="QA-4"><a href="#QA-4" class="headerlink" title="QA"></a>QA</h2><ul>
<li>孪生网络两路输入不一样，是不是很可能引起数值不稳定？<ul>
<li>如果有两种输入，一种是文字，一种是图片，确实很可能输出的范围不一样。这个时候可以使用权重调整或者使用batchnorm。</li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>Multilayer perceptrons</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="http://zhuxz0299.github.io/posts/abee6984.html">http://zhuxz0299.github.io/posts/abee6984.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>朱小志</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2023-09-01</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2023-09-03</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/PyTorch/">PyTorch</a></div><div class="post_share"><div class="social-share" data-image="https://source.fomal.cc/img/default_cover_8.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/posts/379ff448.html" title="Linear network"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://source.fomal.cc/img/default_cover_31.webp" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Linear network</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.</span> <span class="toc-text">多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.1.</span> <span class="toc-text">感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="toc-number">1.1.1.</span> <span class="toc-text">训练感知机</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%B6%E6%95%9B%E5%AE%9A%E7%90%86"><span class="toc-number">1.1.2.</span> <span class="toc-text">收敛定理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#XOR%E9%97%AE%E9%A2%98"><span class="toc-number">1.1.3.</span> <span class="toc-text">XOR问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA-1"><span class="toc-number">1.2.</span> <span class="toc-text">多层感知机</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%95%E9%9A%90%E8%97%8F%E5%B1%82%E2%80%94%E2%80%94%E5%8D%95%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.1.</span> <span class="toc-text">单隐藏层——单分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.2.</span> <span class="toc-text">激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Sigmoid-%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">Sigmoid 激活函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Tanh%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">Tanh函数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ReLU%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">1.2.2.3.</span> <span class="toc-text">ReLU激活函数</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB"><span class="toc-number">1.2.3.</span> <span class="toc-text">多类分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E9%9A%90%E8%97%8F%E5%B1%82"><span class="toc-number">1.2.4.</span> <span class="toc-text">多隐藏层</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#QA"><span class="toc-number">1.3.</span> <span class="toc-text">QA</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9-%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">2.</span> <span class="toc-text">模型选择+过拟合和欠拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9"><span class="toc-number">2.1.</span> <span class="toc-text">模型选择</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E5%92%8C%E6%B3%9B%E5%8C%96%E8%AF%AF%E5%B7%AE"><span class="toc-number">2.1.1.</span> <span class="toc-text">训练误差和泛化误差</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%AA%8C%E8%AF%81%E6%95%B0%E6%8D%AE%E9%9B%86%E5%92%8C%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.1.2.</span> <span class="toc-text">验证数据集和测试数据集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#K-%E5%88%99%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">2.1.3.</span> <span class="toc-text">K-则交叉验证</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">2.2.</span> <span class="toc-text">过拟合和欠拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F"><span class="toc-number">2.2.1.</span> <span class="toc-text">模型容量</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%B0%E8%AE%A1%E6%A8%A1%E5%9E%8B%E5%AE%B9%E9%87%8F"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">估计模型容量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#VC%E7%BB%B4"><span class="toc-number">2.2.1.2.</span> <span class="toc-text">VC维</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-number">2.2.2.</span> <span class="toc-text">数据复杂度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#QA-1"><span class="toc-number">2.3.</span> <span class="toc-text">QA</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80"><span class="toc-number">3.</span> <span class="toc-text">权重衰退</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E9%80%80-1"><span class="toc-number">3.1.</span> <span class="toc-text">权重衰退</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%9D%87%E6%96%B9%E8%8C%83%E6%95%B0%E4%BD%9C%E4%B8%BA%E7%A1%AC%E6%80%A7%E9%99%90%E5%88%B6"><span class="toc-number">3.1.1.</span> <span class="toc-text">使用均方范数作为硬性限制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E5%9D%87%E6%96%B9%E8%8C%83%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%9F%94%E6%80%A7%E9%99%90%E5%88%B6"><span class="toc-number">3.1.2.</span> <span class="toc-text">使用均方范数作为柔性限制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0%E6%B3%95%E5%88%99"><span class="toc-number">3.1.3.</span> <span class="toc-text">参数更新法则</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#QA-2"><span class="toc-number">3.2.</span> <span class="toc-text">QA</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%A2%E5%BC%83%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">丢弃法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%A2%E5%BC%83%E6%B3%95-1"><span class="toc-number">4.1.</span> <span class="toc-text">丢弃法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A0%E5%81%8F%E5%B7%AE%E7%9A%84%E5%8A%A0%E5%85%A5%E5%99%AA%E9%9F%B3"><span class="toc-number">4.1.1.</span> <span class="toc-text">无偏差的加入噪音</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E4%B8%A2%E5%BC%83%E6%B3%95"><span class="toc-number">4.1.2.</span> <span class="toc-text">使用丢弃法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#QA-3"><span class="toc-number">4.2.</span> <span class="toc-text">QA</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7-%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">5.</span> <span class="toc-text">数值稳定性+模型初始化和激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7"><span class="toc-number">5.1.</span> <span class="toc-text">数值稳定性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%A2%AF%E5%BA%A6"><span class="toc-number">5.1.1.</span> <span class="toc-text">神经网络的梯度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">5.1.2.</span> <span class="toc-text">梯度爆炸的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-number">5.1.3.</span> <span class="toc-text">梯度消失的问题</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">5.2.</span> <span class="toc-text">模型初始化和激活函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A9%E6%AF%8F%E5%B1%82%E7%9A%84%E6%96%B9%E5%B7%AE%E6%98%AF%E4%B8%80%E4%B8%AA%E5%B8%B8%E6%95%B0"><span class="toc-number">5.2.1.</span> <span class="toc-text">让每层的方差是一个常数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">5.2.2.</span> <span class="toc-text">权重初始化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Xavier%E5%88%9D%E5%A7%8B"><span class="toc-number">5.2.3.</span> <span class="toc-text">Xavier初始</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%87%E8%AE%BE%E7%BA%BF%E6%80%A7%E7%9A%84%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">5.2.4.</span> <span class="toc-text">假设线性的激活函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A3%80%E6%9F%A5%E5%B8%B8%E7%94%A8%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="toc-number">5.2.5.</span> <span class="toc-text">检查常用激活函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#QA-4"><span class="toc-number">5.3.</span> <span class="toc-text">QA</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 By 朱小志</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script defer src="/js/cursor.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v6.2.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.1" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" data-title="本站采用多线部署，主线路托管于Vercel" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vercel-brightgreen?style=flat&amp;logo=Vercel" alt=""/></a><a class="github-badge" target="_blank" href="https://dashboard.4everland.org/" style="margin-inline:5px" data-title="本站采用多线部署，备用线路托管于4EVERLAND" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-4EVERLAND-22DDDD?style=flat&amp;logo=IPFS" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1s');
    arr[i].setAttribute('data-wow-delay', '500ms');
    arr[i].setAttribute('data-wow-offset', '100');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '');
    arr[i].setAttribute('data-wow-delay', '');
    arr[i].setAttribute('data-wow-offset', '');
    arr[i].setAttribute('data-wow-iteration', '');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax>
    function butterfly_categories_card_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<style>li.categoryBar-list-item{width:32.3%;}.categoryBar-list{max-height: 190px;overflow:auto;}.categoryBar-list::-webkit-scrollbar{width:0!important}@media screen and (max-width: 650px){.categoryBar-list{max-height: 160px;}}</style><div class="recent-post-item" style="height:auto;width:100%;padding:0px;"><div id="categoryBar"><ul class="categoryBar-list"><li class="categoryBar-list-item" style="background:url(https://source.fomal.cc/img/default_cover_121.webp);"> <a class="categoryBar-list-link" href="categories/linux/">linux</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr"></span></li><li class="categoryBar-list-item" style="background:url(https://source.fomal.cc/img/default_cover_122.webp);"> <a class="categoryBar-list-link" href="categories/demo/">demo</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr"></span></li><li class="categoryBar-list-item" style="background:url(https://source.fomal.cc/img/default_cover_123.webp);"> <a class="categoryBar-list-link" href="categories/latex/">latex</a><span class="categoryBar-list-count">5</span><span class="categoryBar-list-descr"></span></li><li class="categoryBar-list-item" style="background:url(https://source.fomal.cc/img/default_cover_124.webp);"> <a class="categoryBar-list-link" href="categories/Operating-system/">Operating system</a><span class="categoryBar-list-count">11</span><span class="categoryBar-list-descr"></span></li><li class="categoryBar-list-item" style="background:url(https://source.fomal.cc/img/default_cover_125.webp);"> <a class="categoryBar-list-link" href="categories/动手学深度学习/">动手学深度学习</a><span class="categoryBar-list-count">2</span><span class="categoryBar-list-descr"></span></li><li class="categoryBar-list-item" style="background:url(https://source.fomal.cc/img/default_cover_126.webp);"> <a class="categoryBar-list-link" href="categories/自行车/">自行车</a><span class="categoryBar-list-count">5</span><span class="categoryBar-list-descr"></span></li><li class="categoryBar-list-item" style="background:url(https://source.fomal.cc/img/default_cover_127.webp);"> <a class="categoryBar-list-link" href="categories/computer-graphics-notes/">computer graphics notes</a><span class="categoryBar-list-count">7</span><span class="categoryBar-list-descr"></span></li><li class="categoryBar-list-item" style="background:url(https://source.fomal.cc/img/default_cover_128.webp);"> <a class="categoryBar-list-link" href="categories/量子力学/">量子力学</a><span class="categoryBar-list-count">13</span><span class="categoryBar-list-descr"></span></li><li class="categoryBar-list-item" style="background:url(undefined);"> <a class="categoryBar-list-link" href="categories/试卷解答/">试卷解答</a><span class="categoryBar-list-count">1</span><span class="categoryBar-list-descr"></span></li></ul></div></div>';
      console.log('已挂载butterfly_categories_card')
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      }
    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    butterfly_categories_card_injector_config()
    }
  </script><!-- hexo injector body_end end --></body></html>